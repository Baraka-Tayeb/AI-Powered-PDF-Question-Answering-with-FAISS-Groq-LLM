# -*- coding: utf-8 -*-
"""Untitled38.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yw1s5rgor8QxPj2J2yPSAERbf435WSDF

## Install Required Libraries
"""

!pip install pymupdf sentence-transformers faiss-cpu langchain-text-splitters groq

"""##  UPLOAD PDF FILE and EXTRACT TEXT FROM PDF PAGES"""

from google.colab import files
import fitz  # PyMuPDF

# 1. Upload the PDF
uploaded = files.upload()
pdf_path = list(uploaded.keys())[0]
print("Using PDF:", pdf_path)

# 2. Extract text from all pages
doc = fitz.open(pdf_path)

all_text = ""
pages_text = []  # keep per-page text if needed

for page in doc:
    page_text = page.get_text()
    pages_text.append(page_text)
    all_text += page_text + "\n"

print("Total pages:", len(pages_text))
print("\nPreview of extracted text:\n")
print(all_text[:1500])

"""## SPLIT EXTRACTED TEXT INTO CHUNKS"""

from langchain_text_splitters import RecursiveCharacterTextSplitter

# Create splitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", ".", " ", ""]
)

# Split into chunks (strings)
chunks = text_splitter.split_text(all_text)

print(f"Total chunks created: {len(chunks)}\n")
print("First chunk preview:")
print("-------------------------------------")
print(chunks[0][:500])

"""## LOAD EMBEDDING MODEL & CREATE EMBEDDINGS"""

from sentence_transformers import SentenceTransformer

# Load free embedding model
model = SentenceTransformer("all-MiniLM-L6-v2")

def embed_text_local(text: str):
    # Returns a 1D vector
    return model.encode([text])[0]

# Create embeddings for all chunks
import numpy as np

embeddings = [embed_text_local(chunk) for chunk in chunks]

print("Embeddings created successfully!")
print("Embedding vector size:", len(embeddings[0]))
print("Total embeddings:", len(embeddings))

"""##BUILD FAISS VECTOR STORE"""

import faiss
import numpy as np

# Convert embeddings list â†’ float32 matrix
embedding_matrix = np.array(embeddings).astype("float32")

dim = embedding_matrix.shape[1]  # should be 384
index = faiss.IndexFlatL2(dim)
index.add(embedding_matrix)

print("FAISS index created!")
print("Index total vectors:", index.ntotal)

"""## DEFINE FAISS SEARCH FUNCTION"""

def search_faiss(query: str, top_k: int = 5):
    # 1. Embed the query
    query_embedding = embed_text_local(query)
    query_vector = np.array(query_embedding).astype("float32").reshape(1, -1)

    # 2. Search in FAISS
    distances, indices = index.search(query_vector, top_k)

    # 3. Return the matching chunks (strings)
    results = []
    for i in indices[0]:
        results.append(chunks[i])
    return results

# Quick test
test_query = "What is this guide about?"
results = search_faiss(test_query, top_k=3)

for i, res in enumerate(results):
    print(f"\n--- Result {i+1} ---\n")
    print(res[:500])

"""##CONNECT TO GROQ LLM API"""

import os

# Paste your key ONLY here, and never share this cell
os.environ["GROQ_API_KEY"] = "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"

from groq import Groq

client = Groq(api_key=os.environ["GROQ_API_KEY"])

response = client.chat.completions.create(
    model="llama-3.1-8b-instant",
    messages=[{"role": "user", "content": "Hello from my Colab RAG project!"}]
)

print(response.choices[0].message.content)

"""## DEFINE RAG ANSWERING FUNCTION"""

def answer_query(query: str, top_k: int = 5) -> str:
    # 1. Retrieve chunks from FAISS
    retrieved_chunks = search_faiss(query, top_k=top_k)
    context = "\n\n---\n\n".join(retrieved_chunks)

    # 2. Strict prompt to reduce hallucinations
    prompt = f"""
You are a STRICT retrieval-augmented assistant.

RULES:
- Use ONLY the information from the CONTEXT below.
- If the answer is not clearly present in the context, reply:
  "The document does not provide this information."
- Do NOT guess or add extra examples.
- Do NOT use outside knowledge.

CONTEXT:
{context}

QUESTION:
{query}

ANSWER (based only on the context):
"""

    # 3. Call Groq LLM
    response = client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content

"""##TEST RAG WITH SAMPLE QUESTIONS"""

print("Q:", "What is the purpose of this guide?")
print("A:", answer_query("What is the purpose of this guide?"), "\n")

print("Q:", "How can someone shop online safely?")
print("A:", answer_query("How can someone shop online safely?"), "\n")

print("Q:", "What are the benefits of online shopping mentioned?")
print("A:", answer_query("What are the benefits of online shopping mentioned?"), "\n")

print("Q:", "What are the next steps after reading this guide?")
print("A:", answer_query("What are the next steps after reading this guide?"), "\n")

"""## QUESTIONS LIST FOR AUTO-EVALUATION"""

#  question list
questions = [
    "What is this guide about?",
    "Who is this guide for?",
    "What does online shopping allow you to do?",
    "What is one benefit of online shopping?",
    "How can someone stay safe when shopping online?",
    "What should you avoid when shopping online?",
    "What payment method can help protect you from fraud?",
    "What can customer reviews help you with?",
    "Why is budgeting easier when shopping online?",
    "Can you send online purchases to someone else?"
]

# Let the model answer each question
for q in easy_questions:
    print("Q:", q)
    print("A:", answer_query(q))
    print("------------------------------------\n")

"""## BUILD SHOPPING GUIDE AGENT CLASS (OOP RAG)"""

class ShoppingGuideAgent:

    def __init__(self, client, index, chunks):
        self.client = client
        self.index = index
        self.chunks = chunks

    def search(self, query, top_k=5):
        """FAISS semantic search"""
        query_embedding = embed_text_local(query)
        query_vector = np.array(query_embedding).astype("float32").reshape(1, -1)
        distances, indices = self.index.search(query_vector, top_k)
        return [self.chunks[i] for i in indices[0]]

    def build_prompt(self, query, context):
        """Build safe RAG prompt"""
        return f"""
You are a helpful AI agent that answers questions ONLY using the provided context.
If the answer is not clearly present, reply exactly with:
"The document does not provide this information."

CONTEXT:
{context}

QUESTION:
{query}

ANSWER:
"""

    def ask(self, query, top_k=5):
        """Answer user questions using Groq + FAISS"""
        retrieved_chunks = self.search(query, top_k)
        context = "\n\n---\n\n".join(retrieved_chunks)
        prompt = self.build_prompt(query, context)

        response = self.client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[{"role": "user", "content": prompt}]
        )

        return response.choices[0].message.content

"""## USE THE AGENT TO ASK QUESTIONS"""

agent = ShoppingGuideAgent(client, index, chunks)

questions = [
    "What is this guide about?",
    "How can I shop online safely?",
    "What is click and collect?",
    "How does online shopping help with budgeting?"
]

for q in questions:
    print(f"Q: {q}")
    print(f"A: {agent.ask(q)}\n")